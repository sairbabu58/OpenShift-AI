Workbenches:
In RHOAI, a workbench is a containerized, cloud-native working environment for data scientists. Workbenches run as OpenShift pods 
and are designed for machine learning and data science. To this end, workbenches include the JupyterLab notebook execution environment, 
standard data science libraries, such as TensorFlow, and GPU acceleration capabilities, among other features. RHOAI ships with preconfigured 
workbench images, which provide data scientists with stable working environments with little setup. The implementation of workbenches 
relies on the Kubeflow upstream project.

Workbench images:
A workbench image is a container image that RHOAI uses to create workbenches. RHOAI includes a number of images ready to use for multiple 
common data science stacks. Each image contains a different set of libraries and versions. Minimal Python, Standard Data Science, PyTorch, 
and TensorFlow are examples of default images included in RHOAI.

Cluster storage
Workbenches run as containers, and containers are designed to be stateless. However, users must be able to retain their work after 
logging out or if the workbench restarts. To make workbenches stateful, RHOAI uses cluster storage, which is a Persistent Volume Claim (PVC) 
mounted in a specific directory of the workbench container. Every workbench includes, at least, one cluster storage mounted in the root 
directory of the workbench.

Data Connections:
In RHOAI, a data connection is a set of configuration parameters for connecting workbenches to S3-compatible storage services. When you 
associate a data connection to a workbench, RHOAI injects the values of the data connection as environment variables into the workbench.

Data Science Pipelines:
In RHOAI, a data science pipeline is a workflow that executes scripts or Jupyter notebooks in an automated way. Data Science Pipelines are 
a key feature in an AI project, considering the experimental nature of data science and machine learning. Pipelines can help data scientists 
operationalize the execution of their experiments and organize the results. For example, a typical pipeline might automate an experiment 
run by gathering data, cleaning data, training a model, evaluating the model, and uploading the model and its evaluation report to S3.

Model:
An AI model, machine learning model, or just model is the main artifact that results from the training phase of a machine learning workflow.
Each data science or ML framework uses its own format to export models. An exported model is typically saved as one or multiple files. 
In most cases, you must use the same framework for both training and using the model. Often, developers also need an HTTP API, or similar, 
to expose the model to the general public.

Model Serving:
In RHOAI, a model server is a component that automatically deploys models. The model server uses a data connection to download the model 
files from an S3 model store. After downloading the model server files into the container, the model server exposes the model via standard 
REST or gRPC APIs. These APIs are often called inference APIs as they provide the inferences captured in the model.

RHOAI uses KServe as the Model Serving platform, and supports model runtimes such as OpenVINO, Triton, 
Text Generation Inference Server (TGIS) and Caikit.

Model Monitoring:
Administrations and engineers might want to observe Prometheus metrics such as the CPU or memory use of your model server. 
Data scientists can inspect potential drifts in the predictions and decisions of the deployed models.






More details, please follow the AI-262 book.

- Ingest data
- Preprocess and explore data
- Train model
- Evaluate model
- Export and upload model
- Pipeline execution
- Serve model
- Monitor model
- Develop and deploy applications
